{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_with_bert_data_cleaned.ipynb",
      "provenance": [],
      "mount_file_id": "1noswg7cKiM_ViVdZ88a_MS4bVCxPoMiv",
      "authorship_tag": "ABX9TyM8tHDa44cTGYhaySDn6Omu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Esdras123/Sentiment_analysis_of_IMDB_reviews/blob/master/sentiment_analysis_with_bert_data_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfZfIMHO_U0t",
        "colab_type": "text"
      },
      "source": [
        "Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovb9jsQN40F5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "bb2fe315-74c8-43b6-e16d-4bc8fe10a306"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckw4a3Ua_TsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import requests  \n",
        "file_url = \"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\"\n",
        "    \n",
        "r = requests.get(file_url, stream = True)  \n",
        "  \n",
        "with open(\"/content/gdrive/My Drive/movies_review.csv\", \"wb\") as file:  \n",
        "    for block in r.iter_content(chunk_size = 1024): \n",
        "         if block:  \n",
        "             file.write(block) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KkbzdPWOwLp",
        "colab_type": "text"
      },
      "source": [
        "Install the bert package for tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky9j5nY6Oz5v",
        "colab_type": "code",
        "outputId": "f1e6ed85-d419-42ed-d8aa-7c0babd69339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.9MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=b67f0ce19fc696dc691592f4592d87b66adde6d385844ffec4a57e938b7937b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=f1c93837964a799e06f800c03bc7cf594f71f587d3186ea4af9f5dd5cffd603b\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=4037fb76b7f1f7f2984922d69e506c7c028461cbd08bdec11be999f9b812e6d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JQLBaE2Po02",
        "colab_type": "code",
        "outputId": "5be8f061-3a8c-460a-ea0c-14219c2b4a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Import modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import bert\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)\n",
        "pd.set_option('display.max_colwidth',1000)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.2.0\n",
            "Hub version:  0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwBkT7TBQ2Rx",
        "colab_type": "text"
      },
      "source": [
        "Observe and preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nblbgHOHQu2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the IMDB Dataset.csv into Pandas dataframe\n",
        "df=pd.read_csv('/content/gdrive/My Drive/movies_review.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZQIg1ATgEll",
        "colab_type": "code",
        "outputId": "ccf2b747-e8dc-40cd-8ad5-fd7cdc214720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My family and I normally do not watch local movies for the simple reason that they are poorly made, they lack the depth, and just not worth our time.&lt;br /&gt;&lt;br /&gt;The trailer of \"Nasaan ka man\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon. The movie exceeded our expectations. The cinematography was very good, the story beautiful and the acting awesome. Jericho Rosales was really very good, so's Claudine Barretto. The fact that I despised Diether Ocampo proves he was effective at his role. I have never been this touched, moved and affected by a local movie before. Imagine a cynic like me dabbing my eyes at the end of the movie? Congratulations to Star Cinema!! Way to go, Jericho and Claudine!!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Believe it or not, this was at one time the worst movie I had ever seen. Since that time, I have seen many more movies that are worse (how is it possible??) Therefore, to be fair, I had to give this movie a 2 out of 10. But it was a tough call.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After some internet surfing, I found the \"Homefront\" series on DVD at ioffer.com. Before anyone gets excited, the DVD set I received was burned by an amateur from home video tapes recorded off of their TV 15 years ago. The resolution and quality are poor. The images look like you would expect old re-recorded video to look. Although the commercials were edited out, the ending credits of each episode still have voice-over announcements for the segway into the ABC news program \"Nightline\", complete with the top news headlines from the early 1990's. Even with the poor image quality, the shows were watch-able and the sound quality was fine.&lt;br /&gt;&lt;br /&gt;To this show's credit, the casting was nearly perfect. Everyone was believable and really looked the part. Their acting was also above average. The role of Jeff Metcalf is played particularly well by Kyle Chandler (most recently seen in the 2005 remake of King Kong). The period costumes were very authentic as were the sets, especially the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>One of the most unheralded great works of animation. Though it makes the most sophisticated use of the \"cut-out\" method of animation (a la \"South Park\"), the real talent behind \"Twice Upon a Time\" are the vocal characterizations, with Lorenzo Music's (Carlton from TV's \"Rhoda\") Woody Allen-ish Ralph-the-all-purpose-Animal being the centerpiece. The \"accidental nightmare\" sequence is doubtless one of the best pieces of animation ever filmed.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It was the Sixties, and anyone with long hair and a hip, distant attitude could get money to make a movie. That's how Michael Sarne, director of this colossal flop, was able to get the job. Sarne is one of the most supremely untalented people ever given a dollar to make a movie. In fact, the whole studio must have been tricked into agreeing to hire a guy who had made exactly one previous film, a terribly precious 60's-hip black and white featurette called Joanna. That film starred the similarly talentless actress/waif Genevieve Waite who could barely speak an entire line without breaking into some inappropriate facial expression or bat-like twitter. Sarne, who was probably incapable of directing a cartoon, never mind a big-budget Hollywood film, was in way over his head. David Giler's book is the best place to go to find out how the faux-infant terrible Sarne was able to pull the wool over everyone's eyes. If there is ever an historical marker which indicates the superficiality and...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review  sentiment\n",
              "0                                                                                                                                                                                                                                                   My family and I normally do not watch local movies for the simple reason that they are poorly made, they lack the depth, and just not worth our time.<br /><br />The trailer of \"Nasaan ka man\" caught my attention, my daughter in law's and daughter's so we took time out to watch it this afternoon. The movie exceeded our expectations. The cinematography was very good, the story beautiful and the acting awesome. Jericho Rosales was really very good, so's Claudine Barretto. The fact that I despised Diether Ocampo proves he was effective at his role. I have never been this touched, moved and affected by a local movie before. Imagine a cynic like me dabbing my eyes at the end of the movie? Congratulations to Star Cinema!! Way to go, Jericho and Claudine!!          1\n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Believe it or not, this was at one time the worst movie I had ever seen. Since that time, I have seen many more movies that are worse (how is it possible??) Therefore, to be fair, I had to give this movie a 2 out of 10. But it was a tough call.          0\n",
              "2  After some internet surfing, I found the \"Homefront\" series on DVD at ioffer.com. Before anyone gets excited, the DVD set I received was burned by an amateur from home video tapes recorded off of their TV 15 years ago. The resolution and quality are poor. The images look like you would expect old re-recorded video to look. Although the commercials were edited out, the ending credits of each episode still have voice-over announcements for the segway into the ABC news program \"Nightline\", complete with the top news headlines from the early 1990's. Even with the poor image quality, the shows were watch-able and the sound quality was fine.<br /><br />To this show's credit, the casting was nearly perfect. Everyone was believable and really looked the part. Their acting was also above average. The role of Jeff Metcalf is played particularly well by Kyle Chandler (most recently seen in the 2005 remake of King Kong). The period costumes were very authentic as were the sets, especially the ...          0\n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             One of the most unheralded great works of animation. Though it makes the most sophisticated use of the \"cut-out\" method of animation (a la \"South Park\"), the real talent behind \"Twice Upon a Time\" are the vocal characterizations, with Lorenzo Music's (Carlton from TV's \"Rhoda\") Woody Allen-ish Ralph-the-all-purpose-Animal being the centerpiece. The \"accidental nightmare\" sequence is doubtless one of the best pieces of animation ever filmed.          1\n",
              "4  It was the Sixties, and anyone with long hair and a hip, distant attitude could get money to make a movie. That's how Michael Sarne, director of this colossal flop, was able to get the job. Sarne is one of the most supremely untalented people ever given a dollar to make a movie. In fact, the whole studio must have been tricked into agreeing to hire a guy who had made exactly one previous film, a terribly precious 60's-hip black and white featurette called Joanna. That film starred the similarly talentless actress/waif Genevieve Waite who could barely speak an entire line without breaking into some inappropriate facial expression or bat-like twitter. Sarne, who was probably incapable of directing a cartoon, never mind a big-budget Hollywood film, was in way over his head. David Giler's book is the best place to go to find out how the faux-infant terrible Sarne was able to pull the wool over everyone's eyes. If there is ever an historical marker which indicates the superficiality and...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkdKsFb1gO2e",
        "colab_type": "code",
        "outputId": "9335a3ba-b4b4-45b3-e8b7-77e2c6e6839e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"The number of rows and columns in the dataset is: {}\".format(df.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of rows and columns in the dataset is: (50000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRBi2VF2igzD",
        "colab_type": "code",
        "outputId": "c2e9c680-59f8-4e66-8dd3-bf2b5f079c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Identify missing values\n",
        "df.apply(lambda x: sum(x.isnull()), axis=0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "review       0\n",
              "sentiment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yBdn_G8itzS",
        "colab_type": "code",
        "outputId": "0f27967c-f445-47ba-cc46-a86a57a8131c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Check the target class balance\n",
        "df[\"sentiment\"].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    25000\n",
              "0    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwOGHgETK3Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re,string,unicodedata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pVUqWHPKJC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "#stopword_list=nltk.corpus.stopwords.words('english')\n",
        "\n",
        "stopword_list = ['in', 'of', 'at', 'a', 'the']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY7oKPxPLPUR",
        "colab_type": "text"
      },
      "source": [
        "Removing html strips and noise text and special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xhio8i2LSPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(denoise_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVReciDjLcct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(remove_special_characters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32E9_xHkMAtN",
        "colab_type": "text"
      },
      "source": [
        "stem the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--a2oZ5EMCN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(simple_stemmer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFdxCo2dMEfL",
        "colab_type": "text"
      },
      "source": [
        "Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLsDMzo_MHGb",
        "colab_type": "code",
        "outputId": "314383cb-aa00-4c48-861a-c98c65336b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#set stopwords to english\n",
        "#stop=set(stopwords.words('english'))\n",
        "stop = set(stopword_list)\n",
        "print(stop)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(remove_stopwords)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'in', 'a', 'of', 'at', 'the'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H56tQl1nizHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
        "MAX_SEQ_LEN=500 # max sequence length\n",
        "\n",
        "def get_masks(tokens):\n",
        "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
        "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        " \n",
        "def get_segments(tokens):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        "\n",
        "def get_ids(tokens, tokenizer):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def create_single_input(sentence, tokenizer, max_len):\n",
        "    \"\"\"Create an input from a sentence\"\"\"\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = stokens[:max_len]\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        " \n",
        "    ids = get_ids(stokens, tokenizer)\n",
        "    masks = get_masks(stokens)\n",
        "    segments = get_segments(stokens)\n",
        "\n",
        "    return ids, masks, segments\n",
        " \n",
        "def convert_sentences_to_features(sentences, tokenizer):\n",
        "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        " \n",
        "    for sentence in tqdm(sentences,position=0, leave=True):\n",
        "      ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
        "      assert len(ids) == MAX_SEQ_LEN\n",
        "      assert len(masks) == MAX_SEQ_LEN\n",
        "      assert len(segments) == MAX_SEQ_LEN\n",
        "      input_ids.append(ids)\n",
        "      input_masks.append(masks)\n",
        "      input_segments.append(segments)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32), \n",
        "          np.asarray(input_masks, dtype=np.int32), \n",
        "          np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "def create_tonkenizer(bert_layer):\n",
        "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
        "    vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
        "    tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ETwviFjjXzw",
        "colab_type": "text"
      },
      "source": [
        "Creation of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4u6qHj_jTCI",
        "colab_type": "code",
        "outputId": "51ae10c9-b10d-4b39-f2d6-ce99abb5480a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "def nlp_model(callable_object):\n",
        "    # Load the pre-trained BERT base model\n",
        "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n",
        "   \n",
        "    # BERT layer three inputs: ids, masks and segments\n",
        "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n",
        "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n",
        "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n",
        "    pooled_output, sequence_output = bert_layer(inputs) # BERT outputs\n",
        "    \n",
        "    # Add a hidden layer\n",
        "    x = Dense(units=768, activation='relu')(pooled_output)\n",
        "    x = Dropout(0.1)(x)\n",
        " \n",
        "    # Add output layer\n",
        "    outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    # Construct a new model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = nlp_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\")\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          590592      keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            1538        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 110,074,371\n",
            "Trainable params: 110,074,370\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx-svrQxmAwe",
        "colab_type": "text"
      },
      "source": [
        "Training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYjuylSVmExc",
        "colab_type": "code",
        "outputId": "a4948444-b864-4aff-d2ed-0a4f5c2525a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "tokenizer = create_tonkenizer(model.layers[3])\n",
        "\n",
        "#transform the litteral values in binary ones, 1 for Positive and 0 for Negative\n",
        "df['sentiment'].replace('positive',1.,inplace=True)\n",
        "df['sentiment'].replace('negative',0.,inplace=True)\n",
        "\n",
        "#shuffle the train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], shuffle = True, test_size=0.20)\n",
        "\n",
        "X_train = convert_sentences_to_features(X_train, tokenizer)\n",
        "X_test = convert_sentences_to_features(X_test, tokenizer)\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test =  to_categorical(y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [02:12<00:00, 302.99it/s]\n",
            "100%|██████████| 10000/10000 [00:32<00:00, 307.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJhMSGI8mH3a",
        "colab_type": "code",
        "outputId": "5f0dc3e1-c8d3-4b7e-f836-defc462b1132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Train the model\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "\n",
        "# Use Adam optimizer to minimize the categorical_crossentropy loss\n",
        "opt = Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=opt, \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the data to the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    verbose = 1)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('nlp_model.h5') "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 2819s 564ms/step - loss: 0.2756 - accuracy: 0.8834 - val_loss: 0.2209 - val_accuracy: 0.9125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfeGpWYo9is3",
        "colab_type": "text"
      },
      "source": [
        "We analyse the performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqBYGHQ39xOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the pretrained nlp_model\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('nlp_model.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXbbtUTu9zAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on test dataset\n",
        "from sklearn.metrics import classification_report\n",
        "pred_test = np.argmax(new_model.predict(X_test), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aCfZ-7x91ky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "cd90e8c9-5351-456d-ec98-23ae81bb191d"
      },
      "source": [
        "print(classification_report(np.argmax(y_test,axis=1), pred_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92      4938\n",
            "           1       0.96      0.87      0.91      5062\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.92      0.91      0.91     10000\n",
            "weighted avg       0.92      0.91      0.91     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsFuJH1w9ncb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}